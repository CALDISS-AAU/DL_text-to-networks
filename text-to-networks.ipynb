{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "- for the final results, it would be like something  like the image I attached\n",
    "(from https://academic.oup.com/joc/article-abstract/63/6/1011/4085988?redirectedFrom=fulltext)\n",
    "\n",
    "- What I would like to illustrate is what is the meaning of the terms like COVID-19 and how the meaning(s) of COVID-19 change as time progresses from Dec. 2019 to April 2020. Would we, for example, identify possible concept clusters that represent unique dimension of meaning of the complex concept. So I assume in terms of the image I posted earlier to you, the answer is the first one, \"a unique word pair...that...occurs within some immediate context\" For instance, the first line is the word pair of \"Dengue fever\" and \"pandemic\" and the second is \"cause\" and \"emergence\"\n",
    "\n",
    "- Another reason that I used Wordij is about the \"drop words\" like prepositions and Wordij already has a txt document which can be revised in terms of our own need to remove prepositions etc. from the corpus. I assume some one should also have that in the case of py.\n",
    "\n",
    "\n",
    "### Work flow\n",
    "\n",
    "1. Import texts\n",
    "2. Split into sentences\n",
    "3. N-gram tokenization\n",
    "4. Preprocessing: No stop words, only certain word types\n",
    "5. Edge-list conversion\n",
    "6. Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing texts\n",
    "\n",
    "Using the novel \"The Strange Case Of Dr. Jekyll And Mr. Hyde\" by Robert Louis Stevenson as example (from the Project Gutenberg: https://gutenberg.org).\n",
    "\n",
    "URL to plain text: https://gutenberg.org/files/43/43-0.txt\n",
    "\n",
    "The text is imported with the front matter and end matter removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "response = requests.get(\"https://gutenberg.org/files/43/43-0.txt\")\n",
    "\n",
    "text_raw = response.text\n",
    "\n",
    "text_regex = re.compile(r\"(?<=\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK).*(?=\\*\\*\\* END OF THIS PROJECT GUTENBERG EBOOK)\", re.DOTALL)\n",
    "text = re.findall(text_regex, text_raw)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split into sentences\n",
    "\n",
    "In order to ensure that words occuring in different sentences (on each side of a period), the text is split into sentences by \".\".\n",
    "\n",
    "To avoid splitting at \"dr.\" and \"mr.\", every \".\" found in \"dr.\" or \"mr.\" (regardless of case) is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_mr = re.compile(r\"(?<=\\bdr|mr)\\.\", re.IGNORECASE)\n",
    "\n",
    "text = re.sub(dr_mr, \"\", text)\n",
    "text_sentences = text.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram tokenization\n",
    "\n",
    "With the text as sentences, the sentences can now be converted to ngrams.\n",
    "\n",
    "The ngram is here used to define the immediate context. The range is set to five (`ngram_range = (5,5)`), meaning words are paired together in pairs of five words next to each other.\n",
    "\n",
    "The output of the code below is a list of 5 word ngrams.\n",
    "\n",
    "By doing this, the assumption is that words occuring within five words of each other are in context of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools\n",
    "\n",
    "vectorizer = CountVectorizer(lowercase = False, ngram_range=(5,5)) # Defining vectorizer\n",
    "analyzer = vectorizer.build_analyzer() # Creating analyzer using vectorizer\n",
    "\n",
    "text_ngrams = [analyzer(sentence) for sentence in text_sentences] # Each sentence is processed using the vectorizer\n",
    "\n",
    "text_ngrams_flat = list(itertools.chain(*text_ngrams)) # Flattening list (instead of being nested as a list of lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing\n",
    "\n",
    "The ngrams are preprocessed with the following steps:\n",
    "\n",
    "- Removing stopwords (A standard list of stop words from `spacy` is used)\n",
    "\n",
    "- Removing punctuation (as defined in `string.punctuation`)\n",
    "\n",
    "- Keeping only certain word types: proper nouns (`PROPN`), adjectives (`ADJ`) and nouns (`NOUN`).\n",
    "\n",
    "- All words are converted to lowercase\n",
    "\n",
    "This step can of course be adjusted and fine-tuned. There is fx no named entity recognition, so names and places are not recognized and will not keep their proper casing (everything is converted to lowercase).\n",
    "\n",
    "It might also be the best tool for this, as Spacy is meant for analyzing full texts, so the POS-tagging may not work properly on ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['strange', 'case', 'dr']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "#!python -m spacy download en_core_web_sm # Downloading english language model for spacy (commented out as it only needs to be done once)\n",
    "nlp = spacy.load(\"en_core_web_sm\") # Setting the language model (english)\n",
    "\n",
    "def tokenizer_custom(text): # Defining custom tokenizer function\n",
    "    doc = nlp(text) # Text is being processed by lanuage model\n",
    "    \n",
    "    tokens = [] # Empty list of tokens to fill in\n",
    "    \n",
    "    pos_tags = ['PROPN', 'ADJ', 'NOUN'] # Part of speech tags to keep\n",
    "    stopwords = list(nlp.Defaults.stop_words) # Stopwords to filter\n",
    "    punctuation = string.punctuation # Punctuation to filter\n",
    "    \n",
    "    for token in doc: # Iterates over each word in the text\n",
    "        \n",
    "        if(token.text.lower() in stopwords or token.text in punctuation): # Skips stop words and punctuation (will not be added to tokens)\n",
    "            continue\n",
    "        \n",
    "        if(token.pos_ in pos_tags): # Checks whether word is the correct POS\n",
    "            tokens.append(token.text.lower()) # Adds to the list of tokens\n",
    "            \n",
    "    return(tokens) # Returns the preprocessed token list\n",
    "            \n",
    "tokenizer_custom(text_ngrams_flat[0]) # Testing on first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_tokens = [tokenizer_custom(ngram) for ngram in text_ngrams_flat] # Applies tokenizer on all sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Edge-list conversion\n",
    "\n",
    "To analyze the tokens as a network, they have to be converted to an edgelist format (a list of unique word pairs).\n",
    "\n",
    "`itertools.combinations` finds unique 2-word combinations in each token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c30d8eab7662>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0medgelists_wordpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0medgelists_wordpairs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0medgelists_wordpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "edgelists_wordpairs = [list(itertools.combinations(tokens,2)) for tokens in ngrams_tokens]\n",
    "edgelists_wordpairs_flat = list(itertools.chain(*edgelists_wordpairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter` is then used to count up the word pairs (remember that because of the previous preprocessing, words have to be in the same sentence and within five words of each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "pairs_counter = Counter(edgelists_wordpairs_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The counted word pairs can be converted to a pandas data frame for a better overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wordpairs_df = pd.DataFrame.from_dict(pairs_counter, orient='index').reset_index().rename(columns={'index': 'wordpair', 0: 'count'})\n",
    "wordpairs_df.sort_values(by = ['count'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Network\n",
    "\n",
    "To convert the word pairs to a network object, `networkx` is used. The word paris are first filtered (minimum 5 occurences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpairs_filter = list(wordpairs_df.loc[wordpairs_df['count'] >= 5, 'wordpair']) # List of word pairs occuring at least 5 times\n",
    "edgelists_wordpairs_filter = [wordpair for wordpair in edgelists_wordpairs_flat if wordpair in wordpairs_filter] # Filtering list of wordpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word pairs are then converted to a network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "G.add_edges_from(edgelists_wordpairs_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The netword can be visualized directly in Python (not that pretty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(24,24)) \n",
    "nx.draw_networkx(G, with_labels = True, node_size=[v * 10 for v in dict(G.degree).values()], edge_color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, the network can be exported as a Gephi-compatible format (like .gexf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(G, \"jekyll-hyde_wordpairs_filter-5.gexf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
